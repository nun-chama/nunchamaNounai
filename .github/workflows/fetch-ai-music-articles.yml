name: Fetch AI Music Articles from Google News

on:
  schedule:
    # 毎日日本時間10:00 (UTC 01:00)
    - cron: '0 1 * * *'
  workflow_dispatch:
    inputs:
      article_count:
        description: 'Number of articles to fetch'
        required: false
        default: '5'

jobs:
  fetch-articles:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install requests beautifulsoup4 feedparser

      - name: Create fetch script
        run: |
          cat << 'EOF' > fetch_articles.py
          import os
          import feedparser
          from datetime import datetime
          import urllib.parse
          from bs4 import BeautifulSoup
          import requests

          def fetch_google_news_rss(query, count=5):
              """Fetch news from Google News RSS"""
              encoded_query = urllib.parse.quote(query)
              rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=ja&gl=JP&ceid=JP:ja"
              
              print(f"Fetching RSS: {rss_url}")
              feed = feedparser.parse(rss_url)
              
              articles = []
              for entry in feed.entries[:count]:
                  # Extract source name
                  source = entry.get('source', {}).get('title', 'Unknown Source')
                  
                  # Published date
                  published = entry.get('published', '')
                  try:
                      # Attempt to convert date format
                      dt = datetime(*entry.published_parsed[:6])
                      date_str = dt.strftime('%Y-%m-%d %H:%M:%S')
                  except:
                      date_str = published

                  articles.append({
                      'title': entry.title,
                      'link': entry.link,
                      'date': date_str,
                      'source': source,
                      'summary': entry.get('summary', '')  # Summary might contain HTML
                  })
              
              return articles

          def clean_html(html_content):
              soup = BeautifulSoup(html_content, 'html.parser')
              return soup.get_text()

          def save_article(article, folder_path, index):
              """Save an article as markdown file"""
              filename = f"article_{index+1:02d}.md"
              filepath = os.path.join(folder_path, filename)
              
              clean_summary = clean_html(article['summary'])
              
              content = f"""# {article['title']}

          ## 情報源
          {article['source']}

          ## 公開日時
          {article['date']}

          ## リンク
          {article['link']}

          ## 概要
          {clean_summary}

          ---
          *取得日時: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
          """
              
              with open(filepath, 'w', encoding='utf-8') as f:
                  f.write(content)
              
              print(f"Saved: {filepath}")
              return filepath

          def main():
              # Get article count from environment or default
              count = int(os.environ.get('ARTICLE_COUNT', '5'))
              
              # Create folder with timestamp
              timestamp = datetime.now().strftime('%Y-%m-%d_%H%M%S')
              folder_path = os.path.join('articles', timestamp)
              os.makedirs(folder_path, exist_ok=True)
              
              # Search query
              query = 'AI音楽'
              
              print(f"Fetching {count} articles about '{query}' from Google News...")
              articles = fetch_google_news_rss(query, count)
              
              if not articles:
                  print("No articles found. Creating placeholder...")
                  # Create a placeholder file if no articles found
                  placeholder_path = os.path.join(folder_path, 'README.md')
                  with open(placeholder_path, 'w', encoding='utf-8') as f:
                      f.write(f"# AI音楽記事取得 - {timestamp}\n\n")
                      f.write("記事の取得に失敗しました、または記事が見つかりませんでした。\n")
                  return
              
              # Save each article
              saved_files = []
              for i, article in enumerate(articles):
                  filepath = save_article(article, folder_path, i)
                  saved_files.append(filepath)
              
              print(f"\nSaved {len(saved_files)} articles to {folder_path}")

          if __name__ == '__main__':
              main()
          EOF

      - name: Fetch articles
        env:
          ARTICLE_COUNT: ${{ github.event.inputs.article_count || '5' }}
        run: |
          python fetch_articles.py

      - name: Commit and push changes
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add articles/
          git diff --staged --quiet || (git commit -m "Add AI music articles - $(date '+%Y-%m-%d %H:%M:%S')" && git push)
